You are an implementation agent. Build a complete, working, fully offline RAG system exactly as specified below. Do not use the internet in any form. Do not reference Hugging Face. Do not download any models, “snapshots,” or data at runtime. Assume all models (LLM + embeddings) are already available locally via Ollama. The system must run end-to-end without errors and must include ingestion/build, query serving API, hybrid retrieval, optional dense retrieval, and a low-weight hypothetical-answer auxiliary retrieval channel. Follow the module and file naming exactly as described.

Overall constraints: The system must operate entirely offline. It must not attempt any network calls other than local loopback to a locally running Ollama server at localhost. It must not import or use Hugging Face libraries. It must not attempt model downloads. All document parsing, chunking, indexing, retrieval, and generation must be done using local libraries and locally available models. The system is intended for QMS-style documents and must enforce strict grounding: the final answer must be supported by retrieved chunks and must refuse to answer if evidence is not found.

Input data: Structured PDF documents in a local folder. Optional DOCX and TXT support is allowed but PDF is primary.

Document processing and chunking: Parse PDFs using local parsing tools (PyMuPDF preferred; pdfminer optional). Convert extracted text to Markdown to preserve structure such as headings, subheadings, lists, tables, and paragraph boundaries. Build a layout-aware document tree from Markdown to preserve hierarchical structure. Apply Docling’s hybrid chunking strategy locally (structure-first, then token control). If Docling hybrid chunker is not available in the environment, implement a compatible local hybrid chunker with the same behavior: initial chunking by headings/sections, then split oversized chunks and merge undersized chunks using token counts computed locally. Do not rely on any tokenizer that triggers downloads. Implement a simple local token counter (whitespace-based) for token control. Each chunk must include metadata: source file name, page number, section path, and content type (paragraph/list/table). Maintain deterministic chunk IDs.

Embeddings and indexes (build time): Generate embeddings for each chunk using Ollama embeddings with the local embedding model bge-small-en-v1.5. Use only the local Ollama embeddings endpoint on localhost. Store embeddings in a FAISS index. Build a BM25 sparse index over chunk texts. Store chunk metadata locally in JSONL or SQLite. Persist all artifacts to STORE_DIR so query-time does not rebuild anything.

Project structure and required files: Create a complete project with these files at minimum: config.yml, utils_qms.py, db_build2.py, ollama_fastapi.py, prompts.py, and any additional helper modules if needed (for example loaders.py, chunking.py, indexing.py), but keep the main logic wired through the required files. Ensure the paths and imports are correct and the system runs without errors.

Configuration management: Runtime configuration must be read from config.yml via load_config() implemented in utils_qms.py. Use config.yml to define DOCS_FOLDER, STORE_DIR, retrieval topN for BM25 and FAISS, final context topK, and weighting parameters for fusion and the hypothetical-answer retrieval channel. Ensure db_build2.py calls load_config() and then ingest_folder(DOCS_FOLDER, STORE_DIR). Make ingest_folder robust to multiple PDFs and create STORE_DIR if missing.

Ingestion build script (db_build2.py): Implement a clean CLI runnable script that reads config, ingests all docs from DOCS_FOLDER, performs parsing, Markdown conversion, document tree construction, hybrid chunking, embeddings, FAISS build, BM25 build, metadata persistence, and writes all artifacts to STORE_DIR. Print clear progress logs and final success message. Ensure idempotence: running it again should rebuild or overwrite indexes cleanly without inconsistent state.

Query server (ollama_fastapi.py): Implement a FastAPI app that loads persisted artifacts from STORE_DIR at startup. Run via uvicorn ollama_fastapi:app. Provide a POST /query endpoint that accepts JSON with a query string. The query pipeline must do primary retrieval and secondary retrieval as described below, build a final context with citations, and call the local Ollama LLM to generate the final answer. Return JSON containing answer text, citations, and optionally debug fields (retrieved chunk IDs and scores) gated by config.

Primary retrieval (runtime): Given a query, perform BM25 retrieval to obtain top N candidate chunks (default 30). Optionally perform dense retrieval by embedding the query using Ollama embeddings and searching FAISS for top N dense matches (config-controlled). Merge BM25 and FAISS results and deduplicate by chunk ID. Combine rankings using reciprocal rank fusion or weighted scoring. Produce a primary candidate set ranked by fused score.

Secondary auxiliary retrieval via hypothetical answer (runtime): After receiving the query, ask the local Ollama LLM to generate a short hypothetical answer in strict QMS style. This is not the final answer; it is a retrieval aid. Use a prompt from prompts.py specifically for hypothetical answer generation. Embed the hypothetical answer using Ollama embeddings and run a FAISS search to retrieve a small number of additional chunks (config-controlled, for example top 5–10). Treat these as secondary evidence and apply an explicit penalty/low weight. The system must ensure these chunks have very low influence on the final context: either reserve only a small number of context slots for them, place them later in the context, and/or downweight their ranking score in fusion. Mark them as “auxiliary” in debug outputs.

Context construction: Build the final context primarily from the primary candidate set and secondarily from the auxiliary hypothetical-answer set. Select topK chunks for final context (for example 8–10). Order chunks for coherence by section path and page number where possible. Include citations for each chunk with file name, page number, and section path. If chunk metadata is missing a field, degrade gracefully. Ensure no duplicate chunk texts are included.

Answer generation (runtime): Use prompts.py to create the final strict grounding prompt. The final prompt must instruct the LLM to answer only using the provided context, include citations, and respond with a refusal message such as “Not available in the document” when evidence is insufficient. The generation must be done using the local Ollama chat/generate endpoint on localhost. The response must include citations aligned with chunk IDs or citation numbers.

Strict refusal behavior: Implement evidence checks. If the retrieved context is low-confidence (for example, fused score below threshold or empty retrieval), the system must refuse rather than hallucinate. Ensure the LLM prompt reinforces this rule. Return refusal response with no fabricated claims.

Offline safety enforcement: Ensure all code paths do not import network-based model loaders. Do not use transformers downloads. Do not call external URLs. Only allow calls to localhost for Ollama. Provide a simple runtime guard that can optionally assert there are no non-local HTTP requests.

Additional deliverable: Also create an SVG or ensure compatibility with the provided SVG flow diagram. If “svg attached” is referenced but not provided, do not block; simply ensure your pipeline matches the described flow and the Mermaid diagram previously produced.

Testing and validation: Provide a minimal local smoke test procedure: run db_build2.py to build indexes, then run uvicorn ollama_fastapi:app, then send a sample POST /query request. Ensure the system returns a grounded answer with citations or a refusal. Add basic error handling for missing STORE_DIR artifacts, empty document folder, and Ollama not running.

Output requirements: Produce the full project files content. Ensure there are no syntax errors, import errors, path errors, or missing functions. Ensure db_build2.py and ollama_fastapi.py run cleanly. Ensure all constraints are respected: fully offline, no downloads, no Hugging Face, local-only Ollama usage, QMS strict grounding, hybrid BM25+FAISS retrieval, plus low-weight hypothetical-answer auxiliary retrieval.
